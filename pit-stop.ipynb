{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f307c26",
   "metadata": {},
   "source": [
    "# 1. Imports, data and template prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas chromadb faiss-cpu sentence-transformers langchain langchain-openai\n",
    "!pip install -qU langchain_community faiss-cpu\n",
    "\n",
    "from typing import List\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacdb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load CSVs\n",
    "parts_df = pd.read_csv(\"./Data/parts.csv\")\n",
    "systems_df = pd.read_csv(\"./Data/systems.csv\")\n",
    "scenarios_df = pd.read_csv(\"./Data/automotive_scenarios.csv\")\n",
    "\n",
    "# Combine them into one list of documents\n",
    "def make_text(row):\n",
    "    return \" | \".join(str(x) for x in row if pd.notnull(x))\n",
    "\n",
    "texts = parts_df.apply(make_text, axis=1).tolist() + \\\n",
    "        systems_df.apply(make_text, axis=1).tolist() + \\\n",
    "        scenarios_df.apply(make_text, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ea86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalServerEmbeddings(Embeddings):\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        response = requests.post(f\"{self.base_url}/embeddings\", json={\"input\": texts})\n",
    "        return [item[\"embedding\"] for item in response.json()[\"data\"]]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        response = requests.post(f\"{self.base_url}/embeddings\", json={\"input\": [text]})\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "embedding = LocalServerEmbeddings(base_url=\"http://localhost:1234/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = [Document(page_content=t) for t in texts]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following context to answer the question.\n",
    "If unsure, say \"I don't know\". Keep answers short. End with: \"Thanks for asking!\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc82d6d",
   "metadata": {},
   "source": [
    "# 2. Testing phi-4 and deepseek r1 with FAISS/Chromadb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "use_faiss = True  # toggle this to switch between FAISS and Chroma\n",
    "\n",
    "if use_faiss:\n",
    "    vectorstore = FAISS.from_documents(splits, embedding)\n",
    "else:\n",
    "    persist_directory = 'chroma_store'\n",
    "    if os.path.exists(persist_directory):\n",
    "        import shutil\n",
    "        shutil.rmtree(persist_directory)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"phi-4\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "question = \"What causes brake failure in vehicles?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a432cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_2 = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"deepseek-r1-distill-qwen-7b\"\n",
    ")\n",
    "\n",
    "qa_chain_2 = RetrievalQA.from_chain_type(\n",
    "    llm_2,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "question = \"What causes brake failure in vehicles?\"\n",
    "result = qa_chain_2({\"query\": question})\n",
    "\n",
    "print(\"Answer:\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118b088",
   "metadata": {},
   "source": [
    "# 3. Compare chromadb and FAISS using the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c71af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare chromadb and faiss answers using deepseek-r1-distill-qwen-7b\n",
    "import os\n",
    "\n",
    "# Chroma db\n",
    "persist_directory = 'chroma_store'\n",
    "        \n",
    "if os.path.exists(persist_directory):\n",
    "    import shutil\n",
    "    shutil.rmtree(persist_directory)\n",
    "    \n",
    "chromadb_vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# FAISS db\n",
    "faiss_vectorstore = FAISS.from_documents(splits, embedding)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"deepseek-r1-distill-qwen-7b\"\n",
    ")\n",
    "\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=chromadb_vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "qa_chain_faiss = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=faiss_vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "question = \"What causes brake failure in vehicles?\"\n",
    "\n",
    "result_chroma = qa_chain_chroma({\"query\": question})\n",
    "result_faiss = qa_chain_faiss({\"query\": question})\n",
    "\n",
    "# Print results for both vector stores\n",
    "print(\"\\n\\Chroma Results:\")\n",
    "print(\"Answer:\", result_chroma[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result_chroma[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:150])\n",
    "\n",
    "print(\"\\n\\nFAISS Results:\")\n",
    "print(\"Answer:\", result_faiss[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result_faiss[\"source_documents\"]:\n",
    "    print(\"-\", doc.page_content[:150])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c2db9",
   "metadata": {},
   "source": [
    "# 4. Create and use a conversational agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7560b8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a360a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Load CSVs\n",
    "parts_df = pd.read_csv(\"./Data/parts.csv\")\n",
    "systems_df = pd.read_csv(\"./Data/systems.csv\")\n",
    "scenarios_df = pd.read_csv(\"./Data/automotive_scenarios.csv\")\n",
    "\n",
    "# Combine them into one list of documents\n",
    "def make_text(row):\n",
    "    return \" | \".join(str(x) for x in row if pd.notnull(x))\n",
    "\n",
    "texts = parts_df.apply(make_text, axis=1).tolist() + \\\n",
    "        systems_df.apply(make_text, axis=1).tolist() + \\\n",
    "        scenarios_df.apply(make_text, axis=1).tolist()\n",
    "        \n",
    "class LocalServerEmbeddings(Embeddings):\n",
    "    def __init__(self, base_url: str):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        response = requests.post(f\"{self.base_url}/embeddings\", json={\"input\": texts})\n",
    "        return [item[\"embedding\"] for item in response.json()[\"data\"]]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        response = requests.post(f\"{self.base_url}/embeddings\", json={\"input\": [text]})\n",
    "        return response.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "embedding = LocalServerEmbeddings(base_url=\"http://localhost:1234/v1\")\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "documents = [Document(page_content=t) for t in texts]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = splitter.split_documents(documents)\n",
    "\n",
    "template = \"\"\"Use the following context to answer the question.\n",
    "If unsure, say \"I don't know\". Keep answers short. End with: \"Thanks for asking!\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Chroma db\n",
    "persist_directory = 'chroma_store'\n",
    "    \n",
    "chromadb_vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio\",\n",
    "    model=\"deepseek-r1-distill-qwen-7b\"\n",
    ")\n",
    "\n",
    "qa_chain_chroma = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=chromadb_vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629bbc9",
   "metadata": {},
   "source": [
    "## Create the agent that will use the RAG tool and a chain to clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387b3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: HumanMessage\n",
      "Content: Hello\n",
      "---\n",
      "Type: AIMessage\n",
      "Content: <think>\n",
      "Okay, so I'm trying to figure out what to do when someone asks me a question that doesn't seem to match any of the available tools. Let's see... The tools provided are 'rag_qa' and 'clarify_llm'. \n",
      "\n",
      "First, I need to understand each tool. The 'rag_qa' function retrieves information from a knowledge base about automotive parts or scenarios. It takes a query as an argument. The 'clarify_llm' uses an LLM to decide if the user needs more clarification on their message.\n",
      "\n",
      "Now, looking at the example given by the user: \"Hello\". That's just a greeting and doesn't ask for any specific information that these tools can provide. \n",
      "\n",
      "I should check if either tool is applicable here. The 'rag_qa' requires a query about automotive parts or systems, which isn't relevant to \"Hello\". Similarly, 'clarify_llm' is meant for determining if a message needs clarification, but in this case, the user just sent a greeting.\n",
      "\n",
      "Since neither tool can handle a simple greeting, I should respond by acknowledging that and perhaps offering help. But according to the rules, if there's no matching tool, I should respond conversationally without using any tools. \n",
      "\n",
      "So, my response would be something like \"Hello! How can I assist you today?\" which is friendly and offers assistance.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import List, Optional\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@tool\n",
    "def rag_qa(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Answer a user’s question by retrieving from the automotive knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        query: A natural‑language question about automotive parts, systems, or scenarios.\n",
    "    Returns:\n",
    "        A concise answer based on retrieved context.\n",
    "    \"\"\"\n",
    "    result = qa_chain_chroma.invoke({\"query\": query})\n",
    "    return result[\"result\"]\n",
    "\n",
    "\n",
    "clarify_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I want to make sure I understand the user's question or request.\n",
    "    \n",
    "    Here is what they said: {query}\n",
    "\n",
    "    First, determine if this message is ambiguous or needs clarification.\n",
    "    If it's clear and specific enough to provide a good response, respond with just: \"NO_CLARIFY\"\n",
    "\n",
    "    If it's ambiguous or missing important details, formulate ONE specific clarifying question that would help you provide a better response.\n",
    "\n",
    "    Your response should be EITHER:\n",
    "    1. \"NO_CLARIFY\" (if no clarification is needed)\n",
    "    OR\n",
    "    2. A single, concise clarifying question (if clarification is needed)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "clarify_chain = clarify_template | llm | StrOutputParser()\n",
    "\n",
    "@tool\n",
    "def clarify_llm(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to decide if the last user message needs clarification.\n",
    "    Returns a follow-up question if needed, otherwise returns an empty string.\n",
    "    \"\"\"\n",
    "    result = clarify_chain.invoke({\"query\": query})\n",
    "    if \"NO_CLARIFY\" in result:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "tools = [\n",
    "    rag_qa,\n",
    "    clarify_llm\n",
    "]\n",
    "\n",
    "memory = MemorySaver()\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    llm, \n",
    "    tools, \n",
    "    checkpointer=memory)\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hello\")]\n",
    "    }, \n",
    "    config\n",
    ")\n",
    "\n",
    "for message in response[\"messages\"]:\n",
    "    print(f\"Type: {type(message).__name__}\")\n",
    "    print(f\"Content: {message.content}\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
